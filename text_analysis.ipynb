{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Likes       Retweets\n",
      "count  398867.000000  398867.000000\n",
      "mean        3.660571       2.171333\n",
      "std        28.014071      18.606119\n",
      "min         0.000000       0.000000\n",
      "25%         0.000000       0.000000\n",
      "50%         0.000000       0.000000\n",
      "75%         1.000000       0.000000\n",
      "max       993.000000     991.000000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"Hurricane_Harvey.xlsx\")\n",
    "print(data[[\"Likes\",\"Retweets\"]].describe())\n",
    "\n",
    "# extracted_data = data[(data[\"Likes\"] >= 3) & (data[\"Retweets\"] >= 2)]\n",
    "# text_corpus = extracted_data[\"Tweet\"]\n",
    "text_corpus = data[\"Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "\n",
    "for text in text_corpus:\n",
    "\n",
    "    no_url = re.sub(r\"http\\S+\", \"\", str(text))\n",
    "    new_corpus.append(no_url)\n",
    "\n",
    "new_corpus = np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14793\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize,\n",
    "                        max_features=  500,\n",
    "                        encoding='utf-8')\n",
    "\n",
    "train_data = tfidf.fit_transform(new_corpus.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics or components\n",
    "num_components=20\n",
    "\n",
    "# Create LDA object\n",
    "model=LatentDirichletAllocation(n_components=num_components)\n",
    "\n",
    "# Fit and Transform SVD model on data\n",
    "lda_matrix = model.fit_transform(train_data)\n",
    "\n",
    "# Get Components \n",
    "lda_components=model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['safe', 'stay', 'path', 'praying', 'hurricaneharvey', 'texas', 'hope']\n",
      "Topic 1:  ['hurricaneharvey', 'harvey2017', 'prayfortexas', 'louisiana', 'harvey', 'watching', 'texas']\n",
      "Topic 2:  ['oil', 'gas', 'prices', 'harvey', 'hurricane', 'god', 'slams']\n",
      "Topic 3:  ['dog', 'jeff_piotrowski', 'periscope', 'harvey', 'food', 'hurricane', 'house']\n",
      "Topic 4:  ['disaster', 'trump', 'hurricane', 'harvey', 'updates', 'major', 'texas']\n",
      "Topic 5:  ['prayers', 'affected', 'thoughts', 'harvey', 'hurricane', 'way', 'hurricaneharvey']\n",
      "Topic 6:  ['mph', 'winds', 'hurricane', 'katrina', 'harvey', '130', 'climate']\n",
      "Topic 7:  ['news', 'hurricane', 'category', 'breaking', 'harvey', 'texas', '4']\n",
      "Topic 8:  ['powerful', 'u', 'know', 'hurricane', 'harvey', 'need', 'emergency']\n",
      "Topic 9:  ['space', 'seen', 'station', 'nasa', 'international', 'cupola', 'com']\n",
      "Topic 10:  ['good', 'got', 'hurricane', 'harvey', 'check', 't', 'evacuate']\n",
      "Topic 11:  ['com', 'twitter', 'pic', 'hurricane', 'harvey', 'hurricaneharvey', 'texas']\n",
      "Topic 12:  ['texas', 'flooding', 'coast', 'harvey', 'hurricane', 'houston', 'catastrophic']\n",
      "Topic 13:  ['corpus', 'christi', 'border', 'open', 'near', 'hurricane', 'harvey']\n",
      "Topic 14:  ['youtube', 'live', 'video', 'hurricane', '_x000d_', 'harvey', 'weather']\n",
      "Topic 15:  ['rain', 'category', 'hurricane', 'storm', 'harvey', '3', 'texas']\n",
      "Topic 16:  ['realdonaldtrump', 'right', 'hurricane', 'harvey', 'arpaio', 'watch', 'hurricaneharvey']\n",
      "Topic 17:  ['like', 't', 'm', 'hurricane', 'harvey', 'don', 'update']\n",
      "Topic 18:  ['s', 'relief', 'hurricane', 'harvey', 'water', 'come', 'days']\n",
      "Topic 19:  ['hurricane', 'harvey', '4', 'landfall', 'makes', 'category', 'help']\n"
     ]
    }
   ],
   "source": [
    "# Print the topics with their terms\n",
    "terms = tfidf.get_feature_names_out()\n",
    "\n",
    "for index, component in enumerate(lda_components):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:7]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
